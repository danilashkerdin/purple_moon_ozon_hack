{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8b35d9-b07a-461e-8991-aa87306537a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be745827-a0c0-4cd9-bf7e-57d8a90d4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_pairs = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_pairs.parquet\")\n",
    "products_all = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ddf8b2-026f-4989-ae85-12f8a679eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_all = products_all[\n",
    "    ['name', 'categories', 'characteristic_attributes_mapping', 'variantid']\n",
    "].dropna()\n",
    "products_all.categories = products_all.categories.apply(lambda x: json.loads(x)[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eddf04f2-7512-4d8d-86d7-67ef3e404725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457036, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fe4f2b-c406-4af0-8f89-f7934a492cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pairs = var_pairs.merge(\n",
    "    products_all.add_suffix('1'),\n",
    "    on=\"variantid1\"\n",
    ").merge(\n",
    "    products_all.add_suffix('2'),\n",
    "    on=\"variantid2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e3c92e-29f6-4463-bdcf-df12a4bb8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identcial_pairs = product_pairs[product_pairs['target'] == 1]\n",
    "various_pairs = product_pairs[product_pairs['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87cdc67-c050-43e3-b1bd-4def9a927f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134989, 9), (171521, 9))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identcial_pairs.shape, various_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce21e390-0227-47a4-8675-8df159fa99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_attr = json.load(open('./count_not_matched_attr_for_various_pairs_norm.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc8434-f991-460c-b639-bda81b2a8394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4bc26d68-8a00-42fe-b5fd-393f050845d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 1.0\n",
      "name1:  Аккумулятор для Huawei Y5 II/Honor 5A (HB4342A1RBC)\n",
      "name2:  Аккумулятор для Huawei Y5 II/Honor 5A (HB4342A1RBC)\n",
      "0.4145160104477428: name1 & attr1: {'ii', 'y5', 'honor', 'для', '5a', 'huawei', 'аккумулятор'}\n",
      "0.04947761376910447: name2 & attr2: {'аккумулятор', 'для'}\n",
      "0.04947761376910447: name1 & attr2: {'аккумулятор', 'для'}\n",
      "0.4145160104477428: name2 & attr1: {'ii', 'y5', 'honor', 'для', '5a', 'huawei', 'аккумулятор'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd = np.random.choice(product_pairs.index)\n",
    "\n",
    "name1=product_pairs.loc[rnd].name1\n",
    "name2=product_pairs.loc[rnd].name2\n",
    "\n",
    "#words_in_name = name1.translate(str.maketrans('', '', '_()')).replace('/',' ').lower().split(' ')\n",
    "\n",
    "s = \"\"\n",
    "s += f\"target: {product_pairs.loc[rnd].target}\\n\"\n",
    "s += f\"name1:  {name1}\\n\"\n",
    "s += f\"name2:  {name2}\\n\"\n",
    "name1_attr2 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name1.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "            ).replace('/',' ').lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name2_attr1 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name2.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "            ).replace('/',' ').lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name2_attr2 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name2.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "        ).replace('/',' ').lower().split(' ') if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name1_attr1 = set([\n",
    "    word\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "])\n",
    "n1_a1 = sum(\n",
    "    importance_attr[attr]\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    ")\n",
    "n2_a2 = sum(\n",
    "    importance_attr[attr]\n",
    "    for word in product_pairs.loc[rnd].name2.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    ")\n",
    "n1_a2 = sum(\n",
    "    importance_attr[attr]\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    ")\n",
    "n2_a1 = sum(\n",
    "    importance_attr[attr]\n",
    "    for word in product_pairs.loc[rnd].name2.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    ")\n",
    "name1_attr1 & name1_attr2\n",
    "\n",
    "s += f\"{n1_a1}: name1 & attr1: {name1_attr1}\\n\"\n",
    "s += f\"{n2_a2}: name2 & attr2: {name2_attr2}\\n\"\n",
    "s += f\"{n1_a2}: name1 & attr2: {name1_attr2}\\n\"\n",
    "s += f\"{n2_a1}: name2 & attr1: {name2_attr1}\\n\"\n",
    "\n",
    "attr1 = (json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    ")).items()\n",
    "\n",
    "attr2 = (json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    ")).items()\n",
    "\n",
    "print(s)\n",
    "\n",
    "#print(f\"attr1:\", attr1)\n",
    "#print(f\"attr2:\", attr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "899430fa-97e4-4844-a61e-62857f785289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 0.0\n",
      "name1:  Клавиатура для ноутбука Lenovo Legion 5-15IMH05H топкейс\n",
      "name2:  Клавиатура для ноутбука Lenovo Legion 5-15ITH6 топкейс\n",
      "name1 & attr1: {'lenovo': 'рекомендовано для'}\n",
      "name2 & attr2: {}\n",
      "name1 & attr2: [('клавиатура', 'тип'), ('для', 'тип'), ('ноутбука', 'тип')]\n",
      "name2 & attr1: [('клавиатура', 'тип'), ('для', 'тип'), ('ноутбука', 'тип'), ('lenovo', 'совместимость'), ('lenovo', 'рекомендовано для'), ('legion', 'совместимость')]\n",
      "\n",
      "attr1: dict_items([('страна-изготовитель', ['китай']), ('совместимость', ['5cb0z26789<br>legion 5-15imh05h (lenovo) 81y6<br>legion 5-15imh05 (lenovo) 82au<br>legion 5-15arh05h laptop (lenovo) 82b1']), ('рекомендовано для', ['lenovo']), ('тип', ['клавиатура для ноутбука']), ('бренд', ['oem']), ('вес товара, г', ['200']), ('гарантийный срок', ['1 год'])])\n",
      "attr2: dict_items([('страна-изготовитель', ['китай']), ('бренд', ['iqzip']), ('тип', ['клавиатура для ноутбука'])])\n"
     ]
    }
   ],
   "source": [
    "rnd = rnd#np.random.choice(product_pairs.index)\n",
    "\n",
    "name1=product_pairs.loc[rnd].name1\n",
    "name2=product_pairs.loc[rnd].name2\n",
    "\n",
    "#words_in_name = name1.translate(str.maketrans('', '', '_()')).replace('/',' ').lower().split(' ')\n",
    "\n",
    "s = \"\"\n",
    "s += f\"target: {product_pairs.loc[rnd].target}\\n\"\n",
    "s += f\"name1:  {name1}\\n\"\n",
    "s += f\"name2:  {name2}\\n\"\n",
    "name1_attr2 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name1.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "            ).replace('/',' ').lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name2_attr1 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name2.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "            ).replace('/',' ').lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name2_attr2 = set(\n",
    "    [\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name2.translate(\n",
    "            str.maketrans('', '', '_()')\n",
    "        ).replace('/',' ').lower().split(' ') if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "name1_attr1 = set([\n",
    "    word\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "])\n",
    "n1_a1 = {\n",
    "    word: attr\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for attr in json.loads(\n",
    "            product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "        ) if word in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "        )[attr]\n",
    "}\n",
    "n2_a2 = {\n",
    "    word: attr\n",
    "    for word in product_pairs.loc[rnd].name2.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for attr in json.loads(\n",
    "            product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "        ) if word in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "        )[attr]\n",
    "}\n",
    "n1_a2 = [\n",
    "    (word, attr)\n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "       for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    "]\n",
    "n2_a1 = [\n",
    "    (word, attr)\n",
    "    for word in product_pairs.loc[rnd].name2.translate(\n",
    "        str.maketrans('', '', '_()\\n')\n",
    "    ).replace('/',' ').lower().split(' ') \n",
    "        for (attr, value) in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "        ).items() if word in ' '.join(value)\n",
    "]\n",
    "\n",
    "\n",
    "s += f\"name1 & attr1: {n1_a1}\\n\"\n",
    "s += f\"name2 & attr2: {n2_a2}\\n\"\n",
    "s += f\"name1 & attr2: {n1_a2}\\n\"\n",
    "s += f\"name2 & attr1: {n2_a1}\\n\"\n",
    "\n",
    "set(n1_a1) & set(n1_a2)\n",
    "\n",
    "attr1 = (json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    ")).items()\n",
    "\n",
    "attr2 = (json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    ")).items()\n",
    "\n",
    "print(s)\n",
    "print(f\"attr1:\", attr1)\n",
    "print(f\"attr2:\", attr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d218478-a1ef-448e-827b-59466e77745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(\n",
    "    [\n",
    "        name1.lower().split()\n",
    "        for name1, attr2 in zip(various_pairs.name1,\n",
    "                                various_pairs.characteristic_attributes_mapping2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772f4f2-d2ce-43cb-af24-a9fc5005bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_not_matched_attr_for_various_pairs = collections.Counter(\n",
    "    attr_name \n",
    "    if (\n",
    "        (\n",
    "            (' '.join(json.loads(attributes1.lower())[attr_name])) != (\n",
    "             ' '.join(json.loads(attributes2.lower())[attr_name]))\n",
    "        ) and (\n",
    "            cosine_similarity(\n",
    "                *vectorizer.fit_transform(\n",
    "                    [\n",
    "                        ' '.join(json.loads(attributes1.lower())[attr_name]) + 'tr',\n",
    "                        ' '.join(json.loads(attributes2.lower())[attr_name]) + 'tr'\n",
    "                    ]\n",
    "                )\n",
    "            ) < 0.5\n",
    "        )\n",
    "    ) else 'matched'\n",
    "    for (attributes1, attributes2) in zip(various_pairs.characteristic_attributes_mapping1, \n",
    "                                          various_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1.lower())) & set(json.loads(attributes2.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7948b-265a-4b5b-8007-21d05b28ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "del count_not_matched_attr_for_various_pairs['matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e866a-e0e7-4e91-b06b-d565c44035d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count_not_matched_attr_for_ident_pairs = collections.Counter(\n",
    "    attr_name \n",
    "      if (\n",
    "        (\n",
    "            (' '.join(json.loads(attributes1.lower())[attr_name])) != (\n",
    "             ' '.join(json.loads(attributes2.lower())[attr_name]))\n",
    "        ) and (\n",
    "            cosine_similarity(\n",
    "                *vectorizer.fit_transform(\n",
    "                    [\n",
    "                        ' '.join(json.loads(attributes1.lower())[attr_name]) + 'tr',\n",
    "                        ' '.join(json.loads(attributes2.lower())[attr_name]) + 'tr'\n",
    "                    ]\n",
    "                )\n",
    "            ) < 0.5\n",
    "        )\n",
    "    ) else 'matched'\n",
    "\n",
    "    for (attributes1, attributes2) in zip(identcial_pairs.characteristic_attributes_mapping1, \n",
    "                                          identcial_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1.lower())) & set(json.loads(attributes2.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd9a27-e207-421b-af18-b7dbae9ab556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del count_not_matched_attr_for_ident_pairs['matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71eb50-e579-4c9a-8d28-20b51d4d902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "count_not_matched_attr_for_ident_pairs_norm = {\n",
    "    attr: freq / attr_freq[attr]\n",
    "    for attr, freq in count_not_matched_attr_for_indent_pairs.items()\n",
    "}\n",
    "count_not_matched_attr_for_ident_pairs_norm = dict(\n",
    "    sorted(count_not_matched_attr_for_ident_pairs_norm.items(),\n",
    "           key=lambda x: x[1],\n",
    "           reverse=True)\n",
    ")\n",
    "count_not_matched_attr_for_various_pairs_norm = {\n",
    "    attr: freq / attr_freq[attr]\n",
    "    for attr, freq in count_not_matched_attr_for_various_pairs.items()\n",
    "}\n",
    "count_not_matched_attr_for_various_pairs_norm = dict(\n",
    "    sorted(count_not_matched_attr_for_various_pairs_norm.items(),\n",
    "           key=lambda x: x[1],\n",
    "           reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864d4ec-16e2-46a5-bf14-e39ca8ecdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(count_not_matched_attr_for_various_pairs_norm.keys())[:50]) & set(list(count_not_matched_attr_for_ident_pairs_norm.keys())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411dc7d-b933-4922-bbde-522e4f545347",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.barh(\n",
    "    list(count_not_matched_attr_for_indent_pairs_norm.keys())[:50], \n",
    "    list(count_not_matched_attr_for_indent_pairs_norm.values())[:50]\n",
    ")\n",
    "fig.savefig('./count_not_matched_attr_for_indent_pairs_norm.png')\n",
    "\n",
    "fig2, (ax2) = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax2.barh(\n",
    "    list(count_not_matched_attr_for_various_pairs_norm.keys())[:50], \n",
    "    list(count_not_matched_attr_for_various_pairs_norm.values())[:50]\n",
    ")\n",
    "fig2.savefig('./count_not_matched_attr_for_various_pairs_norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813ef2-be9f-43e1-abaf-f25b109f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_not_matched_attr_for_various_pairs.json', 'w') as f:\n",
    "    json.dump(count_not_matched_attr_for_various_pairs, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eecf9-a070-4ecd-a4e8-4fa17ea51443",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_not_matched_attr_for_indent_pairs.json', 'w') as f:\n",
    "    json.dump(count_not_matched_attr_for_indent_pairs, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd188e-18ac-40b2-bad3-4fd073c5cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = (\n",
    "    ' '.join(json.loads(products_all.loc[i].characteristic_attributes_mapping.lower())[attr])\n",
    "    for i in products_all.index\n",
    "        for attr in json.loads(products_all.loc[i].characteristic_attributes_mapping.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810da79-951e-42d7-b715-e2ff5441b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd = np.random.choice(various_pairs.index)\n",
    "\n",
    "attrs = set(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())) & set(\n",
    "    json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping2.lower())\n",
    ")\n",
    "\n",
    "if any(attrs):\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    a = {\n",
    "        attr:  cosine_similarity(\n",
    "            *vectorizer.fit_transform(\n",
    "                [\n",
    "                    ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]),\n",
    "                    ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr])\n",
    "                ]\n",
    "            )\n",
    "        ) for attr in attrs if len(\n",
    "            ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr])\n",
    "        ) > 3\n",
    "    }\n",
    "\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9304da34-e484-4414-83fd-0692ef101954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dbf8082-3301-4d2e-aa84-e13d74de82af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m cosine_similarity(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[43mTfidfVectorizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3 гб\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2 гб\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1401\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1401\u001b[0m X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1253\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1251\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    *TfidfVectorizer(\n",
    "        stop_words=[],\n",
    "        min_df=0.1,\n",
    "        max_df=0.2\n",
    "    ).fit_transform(['3 гб', '2 гб'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a524500-afa5-4438-a5aa-cd39650338d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    vectorizer = TfidfVectorizer(min_df=0.6)\n",
    "\n",
    "    for attr in attrs:\n",
    "        print(' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]), '+++',\n",
    "              ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr]))\n",
    "        print(vectorizer.fit_transform(\n",
    "                [\n",
    "                    ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]),\n",
    "                    ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr])\n",
    "                ]\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70828d08-db1e-461a-8a2e-a743570c8f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
