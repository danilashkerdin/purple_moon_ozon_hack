{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:10:23.678400Z",
     "start_time": "2023-05-19T06:10:16.503064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from catboost import CatBoostClassifier, Pool\n",
    "#from catboost.utils import eval_metric\n",
    "\n",
    "#from transformers import BertModel, BertTokenizer\n",
    "#import torch\n",
    "\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:12:05.012452Z",
     "start_time": "2023-05-19T06:11:27.878101Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairs = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_pairs.parquet\")\n",
    "products_all = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:12:07.891757Z",
     "start_time": "2023-05-19T06:12:07.868318Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "products_all.categories = products_all.categories.apply(lambda x: json.loads(x)[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:12:20.476738Z",
     "start_time": "2023-05-19T06:12:20.458918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairs[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = products_all[['categories', 'variantid']].groupby(\n",
    "    'categories'\n",
    ").count(\n",
    ").sort_values(\n",
    "    by=\"variantid\",\n",
    "    ascending=0\n",
    ")\n",
    "\n",
    "fig, (ax) = plt.subplots(1, 1, figsize=(10, 40))\n",
    "\n",
    "ax.barh(c.index, c.variantid.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:12:27.245184Z",
     "start_time": "2023-05-19T06:12:27.228294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "products = products_all[products_all.categories == \"Смартфоны, планшеты, мобильные телефоны\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get raw data for each variantid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:13:06.702876Z",
     "start_time": "2023-05-19T06:13:03.688111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_pairs = pairs.merge(\n",
    "    products.add_suffix('1'),\n",
    "    on=\"variantid1\"\n",
    ").merge(\n",
    "    products.add_suffix('2'),\n",
    "    on=\"variantid2\"\n",
    ").dropna(subset=[\n",
    "    'name1', 'name2',\n",
    "    'characteristic_attributes_mapping1',\n",
    "    'characteristic_attributes_mapping2'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identcial_pairs = product_pairs[product_pairs['target'] == 1]\n",
    "various_pairs = product_pairs[product_pairs['target'] == 0]\n",
    "\n",
    "identcial_pairs.shape[0], various_pairs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_set3 = [\n",
    "    'variantid1', \n",
    "    'name1', \n",
    "    'characteristic_attributes_mapping1',\n",
    "    'variantid2',\n",
    "    'name2', \n",
    "    'characteristic_attributes_mapping2',\n",
    "    'target'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sentence1 = \"Зарядное устройство\\nКабель USB - Lightning\"\n",
    "sentence2 = \"Зарядное устройство\\nКабель USB - Type-C\"\n",
    "\n",
    "encoded_sentence1 = tokenizer.encode_plus(sentence1, add_special_tokens=True, return_tensors='pt')\n",
    "encoded_sentence2 = tokenizer.encode_plus(sentence2, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output1 = model(encoded_sentence1['input_ids'], attention_mask=encoded_sentence1['attention_mask'])\n",
    "    model_output2 = model(encoded_sentence2['input_ids'], attention_mask=encoded_sentence2['attention_mask'])\n",
    "embeddings1 = model_output1[0][:, 0, :]\n",
    "embeddings2 = model_output2[0][:, 0, :]\n",
    "\n",
    "print(cosine_similarity(embeddings1, embeddings2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\n",
    "    (\n",
    "        sum([\n",
    "            len(set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping1\n",
    "                    )[key]\n",
    "                ) & set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping2\n",
    "                    )[key]\n",
    "                )\n",
    "            ) for key in list(\n",
    "                set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping1\n",
    "                    )\n",
    "                ) & set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping2\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ]) / max(\n",
    "            sum(len(v) for v in json.loads(product_pairs.loc[i].characteristic_attributes_mapping1).values()), \n",
    "            sum(len(v) for v in json.loads(product_pairs.loc[i].characteristic_attributes_mapping2).values())\n",
    "        )\n",
    "    ) for i in np.random.choice(various_pairs.index, 5000)\n",
    "]\n",
    "\n",
    "b = [\n",
    "    (\n",
    "        sum([\n",
    "            len(set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping1\n",
    "                    )[key]\n",
    "                ) & set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping2\n",
    "                    )[key]\n",
    "                )\n",
    "            ) for key in list(\n",
    "                set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping1\n",
    "                    )\n",
    "                ) & set(\n",
    "                    json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping2\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ]) / max(\n",
    "            sum(len(v) for v in json.loads(product_pairs.loc[i].characteristic_attributes_mapping1).values()), \n",
    "            sum(len(v) for v in json.loads(product_pairs.loc[i].characteristic_attributes_mapping2).values())\n",
    "        )\n",
    "    ) for i in np.random.choice(identcial_pairs.index, 5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_ylim(0,700)\n",
    "ax2.set_ylim(0,700)\n",
    "\n",
    "ax1.hist(a, bins=100)\n",
    "ax1.set_title('совпадение аттрибутов различных товаров\\n из категории \"смартфоны\"')\n",
    "\n",
    "ax2.hist(b, bins=100)\n",
    "ax2.set_title('совпадение аттрибутов одинаковых товаров\\n из категории \"смартфоны\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_pairs.loc[rnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.choice(various_pairs.index)\n",
    "a1 = json.loads(\n",
    "    various_pairs.loc[rnd].characteristic_attributes_mapping1\n",
    ")\n",
    "a2 = json.loads(\n",
    "    various_pairs.loc[rnd].characteristic_attributes_mapping2\n",
    ")\n",
    "(various_pairs.loc[rnd].name1, various_pairs.loc[rnd].name2),\n",
    "\n",
    "# a1, a2, {\n",
    "#     key: set(a1[key]) & set(a2[key])\n",
    "#     for key in list(set(a1) & set(a2))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in zip([1,2,3], [2,34,5], [2,1,3])]\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_attr = collections.Counter(\n",
    "    attr_name if json.loads(attributes1)[attr_name] != json.loads(attributes2)[attr_name] else 'matched'\n",
    "    for (attributes1, attributes2) in zip(various_pairs.characteristic_attributes_mapping1, \n",
    "                                          various_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1)) & set(json.loads(attributes2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1, 1, figsize=(20, 40))\n",
    "\n",
    "ax.barh(\n",
    "    list(dict(importance_attr.most_common()).keys())[1:], \n",
    "    list(dict(importance_attr.most_common()).values())[1:]\n",
    ")\n",
    "fig.savefig('./tmp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_counter = collections.Counter(\n",
    "    attr_name \n",
    "    for attributes in products.characteristic_attributes_mapping\n",
    "        for attr_name in json.loads(attributes).keys()\n",
    ")\n",
    "\n",
    "fig, (ax) = plt.subplots(1, 1, figsize=(10, 40))\n",
    "\n",
    "ax.barh(\n",
    "    list(dict(share_counter.most_common()).keys()), \n",
    "    dict(share_counter.most_common()).values()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.choice(product_pairs.index)\n",
    "\n",
    "name1=product_pairs.loc[rnd].name1\n",
    "name2=product_pairs.loc[rnd].name2\n",
    "\n",
    "words_in_name = name.translate(str.maketrans('', '', '_()')).replace('/',' ').lower().split(' ')\n",
    "\n",
    "s = \"\"\n",
    "s += f\"target: {product_pairs.loc[rnd].target}\\n\"\n",
    "s += f\"name1:  {name1}\\n\"\n",
    "s += f\"name2:  {name2}\\n\"\n",
    "name1_attr2 = set([\n",
    "    word \n",
    "    for word in product_pairs.loc[rnd].name1.translate(\n",
    "        str.maketrans('', '', '_()')\n",
    "        ).replace('/',' ').lower().split(' ') if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "s += f\"name1 & attr2: {name1_attr2}\\n\"\n",
    "\n",
    "name1_attr1 = set([\n",
    "        word \n",
    "        for word in product_pairs.loc[rnd].name1.translate(str.maketrans('', '', '_()')\n",
    "                                                          ).replace('/',' '\n",
    "                                                          ).lower(\n",
    "                                                          ).split(' '\n",
    "                                                          ) if any(\n",
    "            word in sentence \n",
    "            for sentence in [\n",
    "                ' '.join(l)\n",
    "                for l in json.loads(\n",
    "                    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "                ).values()\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "s += f\"name1 & attr1 : {name1_attr1}\\n\"\n",
    "\n",
    "attr1 = json.dumps(json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "))\n",
    "\n",
    "attr2 = json.dumps(json.loads(\n",
    "    product_pairs.loc[rnd].characteristic_attributes_mapping2.lower()\n",
    "))\n",
    "\n",
    "#s += f\"attr1: {attr1}\\n\"\n",
    "#s += f\"attr2: {attr2}\\n\"\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.choice(product_pairs.index)\n",
    "\n",
    "name=product_pairs.loc[rnd].name1\n",
    "\n",
    "words_in_name = name.translate(str.maketrans('', '', '_()')).replace('/',' ').lower().split(' ')\n",
    "\n",
    "1=[\n",
    "    (\n",
    "        len(set([\n",
    "            word \n",
    "            for word in product_pairs.loc[i].name1.translate(str.maketrans('', '', '_()')\n",
    "                                                            ).replace('/',' '\n",
    "                                                            ).lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping2.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "            )\n",
    "        ]) & set(words_in_name))/len(words_in_name)\n",
    "    ) for i in np.random.choice(identcial_pairs.index, 3000)\n",
    "]\n",
    "\n",
    "\n",
    "b=[\n",
    "    (\n",
    "        len(set([\n",
    "            word \n",
    "            for word in product_pairs.loc[i].name1.translate(str.maketrans('', '', '_()')\n",
    "                                                            ).replace('/',' '\n",
    "                                                            ).lower().split(' ') if any(\n",
    "                word in sentence \n",
    "                for sentence in [\n",
    "                    ' '.join(l)\n",
    "                    for l in json.loads(\n",
    "                        product_pairs.loc[i].characteristic_attributes_mapping1.lower()\n",
    "                    ).values()\n",
    "                ]\n",
    "            )\n",
    "        ]) & set(words_in_name))/len(words_in_name)\n",
    "    ) for i in np.random.choice(various_pairs.index, 3000)\n",
    "]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#ax1.set_ylim(0,700)\n",
    "#ax2.set_ylim(0,700)\n",
    "\n",
    "ax1.hist(a, bins=10)\n",
    "#ax1.set_title('совпадение аттрибутов различных товаров\\n из категории \"смартфоны\"')\n",
    "\n",
    "ax2.hist(b, bins=10)\n",
    "#ax2.set_title('совпадение аттрибутов одинаковых товаров\\n из категории \"смартфоны\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([\n",
    "    word \n",
    "    for word in words_in_name if any(\n",
    "        word in sentence \n",
    "        for sentence in [\n",
    "            ' '.join(l)\n",
    "            for l in json.loads(\n",
    "                product_pairs.loc[rnd].characteristic_attributes_mapping1.lower()\n",
    "            ).values()\n",
    "        ]\n",
    "    )\n",
    "]) & set(words_in_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.choice(product_pairs.index)\n",
    "\n",
    "print(product_pairs.loc[rnd].target)\n",
    "{\n",
    "    product_pairs.loc[rnd].name1 : (\n",
    "        json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1)\n",
    "    ),\n",
    "    product_pairs.loc[rnd].name2 : (\n",
    "        json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping2)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(\n",
    "    [\n",
    "        len(set(a1[key]) & set(a2[key])) \n",
    "        for key in list(set(a1) & set(a2))\n",
    "    ]\n",
    ") / max(\n",
    "    sum(len(v) for v in a1.values()), \n",
    "    sum(len(v) for v in a2.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pairs.name1.shape[0] - product_pairs.name1.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncf = various_pairs[\n",
    "    ['characteristic_attributes_mapping1','characteristic_attributes_mapping2']\n",
    "]\n",
    "ncf = ncf.dropna()\n",
    "ncf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(cat_not_match), len(ncat_not_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(various_pairs.categories1 != various_pairs.categories2).values.nonzero()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(identcial_pairs.categories1 != identcial_pairs.categories2).values.nonzero()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collusions = identcial_pairs[identcial_pairs.categories1 != identcial_pairs.categories2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_bert = various_pairs[['name_bert_641', 'name_bert_642']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identcial_pairs['similarity'] = [ \n",
    "    cosine_similarity(\n",
    "        identcial_pairs.loc[i].name_bert_641.reshape(1,-1),\n",
    "        identcial_pairs.loc[i].name_bert_642.reshape(1,-1)\n",
    "    )[0][0]\n",
    "    for i in identcial_pairs.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_pairs['similarity'] = [ \n",
    "    cosine_similarity(\n",
    "        various_pairs.loc[i].name_bert_641.reshape(1,-1),\n",
    "        various_pairs.loc[i].name_bert_642.reshape(1,-1)\n",
    "    )[0][0] \n",
    "    for i in various_pairs.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(0.98, 1)\n",
    "ax.set_ylim(0, 6000)\n",
    "\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5)\n",
    "ax.hist(identcial_pairs.similarity, bins=1000, color='r')\n",
    "\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5)\n",
    "ax.hist(various_pairs.similarity, bins=1000, color=\"black\", alpha=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(identcial_pairs.index, \n",
    "            identcial_pairs['similarity'],\n",
    "            linewidths=0.5\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(various_pairs.index,\n",
    "            various_pairs['similarity'],\n",
    "            linewidths=0.5\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loadsimilarityusions.loc[59].characteristic_attributes_mapping1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(collusions.loc[59].characteristic_attributes_mapping2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collusions.loc[59].name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_pairs[various_pairs.categories1 == various_pairs.cat egories2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_pairs[various_pairs.categories1 == various_pairs.categories2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(product_pairs.loc[60].variantid1, product_pairs.loc[60].variantid2, product_pairs.loc[60].target),\\\n",
    "(product_pairs.loc[61].variantid1, product_pairs.loc[61].variantid2, product_pairs.loc[61].target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(product_pairs.loc[60].name1, product_pairs.loc[60].name2),\\\n",
    "(product_pairs.loc[61].name1, product_pairs.loc[61].name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(product_pairs.loc[60].categories1, product_pairs.loc[60].categories2),\\\n",
    "(product_pairs.loc[61].categories1, product_pairs.loc[61].categories2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity60 = cosine_similarity(\n",
    "    product_pairs.loc[60].main_pic_embeddings_resnet_v11[0].reshape(1,-1),\n",
    "    product_pairs.loc[60].main_pic_embeddings_resnet_v12[0].reshape(1,-1)\n",
    ")[0][0]\n",
    "similarity61 = cosine_similarity(\n",
    "    product_pairs.loc[61].main_pic_embeddings_resnet_v11[0].reshape(1,-1),\n",
    "    product_pairs.loc[61].main_pic_embeddings_resnet_v12[0].reshape(1,-1)\n",
    ")[0][0]\n",
    "similarity60, similarity61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(product_pairs.loc[60].characteristic_attributes_mapping1),\\\n",
    "json.loads(product_pairs.loc[60].characteristic_attributes_mapping2),\\\n",
    "json.loads(product_pairs.loc[61].characteristic_attributes_mapping1),\\\n",
    "json.loads(product_pairs.loc[61].characteristic_attributes_mapping2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.categories1.loc[60],\\\n",
    "features.categories2.loc[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.categories1.loc[61],\\\n",
    "features.categories2.loc[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(ncf.characteristic_attributes_mapping1[0]),\\\n",
    "json.loads(ncf.characteristic_attributes_mapping2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(\n",
    "    map(\n",
    "        tuple,\n",
    "        json.loads(\n",
    "            ncf.characteristic_attributes_mapping1[0]\n",
    "        ).values()\n",
    "    )\n",
    ") & set(\n",
    "    map(\n",
    "        tuple,\n",
    "        json.loads(\n",
    "            ncf.characteristic_attributes_mapping2[0]\n",
    "        ).values()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(map(tuple, json.loads(\n",
    "    cf.characteristic_attributes_mapping1[2]\n",
    ").values())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "match_char = [\n",
    "    (\n",
    "        len(set(map(tuple,\n",
    "                    json.loads(\n",
    "                        cf.characteristic_attributes_mapping1[i]\n",
    "                    ).values()\n",
    "                )\n",
    "            ) & set(map(tuple,\n",
    "                    json.loads(\n",
    "                        cf.characteristic_attributes_mapping2[i]\n",
    "                    ).values()\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        min(\n",
    "            len(json.loads(cf.characteristic_attributes_mapping1[i]).keys()),\n",
    "            len(json.loads(cf.characteristic_attributes_mapping2[i]).keys())\n",
    "        )\n",
    "    ) for i in cf.index\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "match_char[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean([(x-y)**2 for x,y in match_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_match_char = [\n",
    "    (\n",
    "        len(set(map(tuple,\n",
    "                    json.loads(\n",
    "                        ncf.characteristic_attributes_mapping1[i]\n",
    "                    ).values()\n",
    "                )\n",
    "            ) & set(map(tuple,\n",
    "                    json.loads(\n",
    "                        ncf.characteristic_attributes_mapping2[i]\n",
    "                    ).values()\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        min(\n",
    "            len(json.loads(ncf.characteristic_attributes_mapping1[i]).keys()),\n",
    "            len(json.loads(ncf.characteristic_attributes_mapping2[i]).keys())\n",
    "        )\n",
    "    ) for i in ncf.index\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([(x-y)**2 for x,y in neg_match_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_match_char[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:13:09.645490Z",
     "start_time": "2023-05-19T06:13:09.620414Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:14:03.019463Z",
     "start_time": "2023-05-19T06:14:03.002869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pic_features(main_pic_embeddings_1,\n",
    "                     main_pic_embeddings_2,\n",
    "                     percentiles: List[int]):\n",
    "    \"\"\"Calculate distances percentiles for \n",
    "    pairwise pic distances. Percentiles are useful \n",
    "    when product has several pictures.\n",
    "    \"\"\"\n",
    "    \n",
    "    if main_pic_embeddings_1 is not None and main_pic_embeddings_2 is not None:\n",
    "        main_pic_embeddings_1 = np.array([x for x in main_pic_embeddings_1])\n",
    "        main_pic_embeddings_2 = np.array([x for x in main_pic_embeddings_2])\n",
    "        \n",
    "        dist_m = pairwise_distances(\n",
    "            main_pic_embeddings_1, main_pic_embeddings_2\n",
    "        )\n",
    "    else:\n",
    "        dist_m = np.array([[-1]])\n",
    "\n",
    "    pair_features = []\n",
    "    pair_features += np.percentile(dist_m, percentiles).tolist()\n",
    "\n",
    "    return pair_features\n",
    "\n",
    "\n",
    "def text_dense_distances(ozon_embedding, comp_embedding):\n",
    "    \"\"\"Calculate Euclidean and Cosine distances between\n",
    "    ozon_embedding and comp_embedding.\n",
    "    \"\"\"\n",
    "    pair_features = []\n",
    "    if ozon_embedding is None or comp_embedding is None:\n",
    "        pair_features = [-1, -1]\n",
    "    elif len(ozon_embedding) == 0 or len(comp_embedding) == 0:\n",
    "        pair_features = [-1, -1]\n",
    "    else:\n",
    "        pair_features.append(\n",
    "            euclidean(ozon_embedding, comp_embedding)\n",
    "        )\n",
    "        cosine_value = cosine(ozon_embedding, comp_embedding)\n",
    "        \n",
    "        pair_features.append(cosine_value)\n",
    "\n",
    "    return pair_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:14:25.968001Z",
     "start_time": "2023-05-19T06:14:25.965711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_pic_features_func = partial(\n",
    "    get_pic_features,\n",
    "    percentiles=[0, 25, 50]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:17:05.936723Z",
     "start_time": "2023-05-19T06:14:27.944747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[[\"pic_dist_0_perc\", \"pic_dist_25_perc\", \"pic_dist_50_perc\"]] = (\n",
    "    features[[\"pic_embeddings_resnet_v11\", \"pic_embeddings_resnet_v12\"]].apply(\n",
    "        lambda x: pd.Series(get_pic_features_func(*x)), axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# btw try to add distances between main pic embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:18:48.810761Z",
     "start_time": "2023-05-19T06:17:26.418017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[[\"euclidean_name_bert_dist\", \"cosine_name_bert_dist\"]] = (\n",
    "    features[[\"name_bert_641\", \"name_bert_642\"]].apply(\n",
    "        lambda x: pd.Series(text_dense_distances(*x)), axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# try to use your favorite NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:28:28.028773Z",
     "start_time": "2023-05-19T06:28:27.086139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[\"cat3\"] = features[\"categories1\"].apply(lambda x: json.loads(x)[\"3\"])\n",
    "cat3_counts = features[\"cat3\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find good cat size threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:28:30.630458Z",
     "start_time": "2023-05-19T06:28:30.624296Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cntr = 0\n",
    "for cat3 in cat3_counts:\n",
    "    if cat3_counts[cat3] < 1_000:\n",
    "        cntr += cat3_counts[cat3]\n",
    "        \n",
    "cntr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10k for \"rest\" cats probably is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:28:36.996792Z",
     "start_time": "2023-05-19T06:28:36.896447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[\"cat3_grouped\"] = features[\"cat3\"].apply(lambda x: x if cat3_counts[x] > 1000 else \"rest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:28:45.696151Z",
     "start_time": "2023-05-19T06:28:45.692484Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = [\"pic_dist_0_perc\", \"pic_dist_25_perc\", \"pic_dist_50_perc\", \n",
    "         \"euclidean_name_bert_dist\", \"cosine_name_bert_dist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:29:01.923278Z",
     "start_time": "2023-05-19T06:28:54.101554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(\n",
    "    features[feats + [\"target\", \"variantid1\", \"variantid2\", \"cat3_grouped\"]], \n",
    "    test_size=0.1, random_state=42, stratify=features[[\"target\", \"cat3_grouped\"]]\n",
    ")\n",
    "\n",
    "X_train, X_val = train_test_split(\n",
    "    X_train[feats + [\"target\",  \"variantid1\", \"variantid2\", \"cat3_grouped\"]], \n",
    "    test_size=0.1, random_state=42, stratify=X_train[[\"target\", \"cat3_grouped\"]]\n",
    ")\n",
    "\n",
    "y_test = X_test[[\"target\", \"variantid1\", \"variantid2\"]]\n",
    "X_test = X_test.drop([\"target\"], axis=1)\n",
    "\n",
    "y_train = X_train[\"target\"]\n",
    "y_val = X_val[\"target\"]\n",
    "\n",
    "X_train = X_train.drop([\"target\"], axis=1)\n",
    "X_val = X_val.drop([\"target\"], axis=1)\n",
    "\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_train[feats],\n",
    "    label=y_train,\n",
    ")\n",
    "eval_pool = Pool(\n",
    "    data=X_val[feats],\n",
    "    label=y_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:29:24.898318Z",
     "start_time": "2023-05-19T06:29:07.544816Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CatBoostClassifier()\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=eval_pool,\n",
    "    plot=True,\n",
    "    verbose=True,\n",
    "    use_best_model=True,\n",
    "    early_stopping_rounds=50,\n",
    "    metric_period=100\n",
    ")\n",
    "model.save_model(f\"./models/baseline.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:30:10.884083Z",
     "start_time": "2023-05-19T06:30:10.873141Z"
    },
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def pr_auc_macro(\n",
    "    target_df: pd.DataFrame,\n",
    "    predictions_df: pd.DataFrame,\n",
    "    prec_level: float = 0.75,\n",
    "    cat_column: str = \"cat3_grouped\"\n",
    ") -> float:\n",
    "    \n",
    "    df = target_df.merge(predictions_df, on=[\"variantid1\", \"variantid2\"])\n",
    "    \n",
    "    y_true = df[\"target\"]\n",
    "    y_pred = df[\"scores\"]\n",
    "    categories = df[cat_column]\n",
    "    \n",
    "    weights = []\n",
    "    pr_aucs = []\n",
    "\n",
    "    unique_cats, counts = np.unique(categories, return_counts=True)\n",
    "\n",
    "    for i, category in enumerate(unique_cats):\n",
    "        cat_idx = np.where(categories == category)[0]\n",
    "        y_pred_cat = y_pred[cat_idx]\n",
    "        y_true_cat = y_true[cat_idx]\n",
    "\n",
    "        y, x, thr = precision_recall_curve(y_true_cat, y_pred_cat)\n",
    "        gt_prec_level_idx = np.where(y >= prec_level)[0]\n",
    "\n",
    "        try:\n",
    "            pr_auc_prec_level = auc(x[gt_prec_level_idx], y[gt_prec_level_idx])\n",
    "            if not np.isnan(pr_auc_prec_level):\n",
    "                pr_aucs.append(pr_auc_prec_level)\n",
    "                weights.append(counts[i] / len(categories))\n",
    "        except ValueError as err:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(0)\n",
    "    return np.average(pr_aucs, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:30:13.294341Z",
     "start_time": "2023-05-19T06:30:13.258507Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test[\"scores\"] = model.predict_proba(X_test[feats])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:30:28.980002Z",
     "start_time": "2023-05-19T06:30:28.331803Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pr_auc_macro_metr = pr_auc_macro(\n",
    "    target_df=y_test, \n",
    "    predictions_df=X_test,\n",
    "    prec_level=0.75,\n",
    "    cat_column=\"cat3_grouped\"\n",
    ")\n",
    "\n",
    "pr_auc_macro_metr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:30:42.074029Z",
     "start_time": "2023-05-19T06:30:41.619959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall, thrs = precision_recall_curve(y_test[\"target\"], X_test[\"scores\"])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(15, 7))\n",
    "\n",
    "ax1.plot(recall, precision)\n",
    "ax1.axhline(y=0.75, color='grey', linestyle='-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:31:32.836414Z",
     "start_time": "2023-05-19T06:31:31.181747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pairs = pd.read_parquet(\"hackathon_files_for_participants_ozon/test_pairs_wo_target.parquet\")\n",
    "test_etl = pd.read_parquet(\"hackathon_files_for_participants_ozon/test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the same features as for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:31:44.061466Z",
     "start_time": "2023-05-19T06:31:44.008543Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features = (\n",
    "    test_pairs\n",
    "    .merge(\n",
    "        test_etl\n",
    "        .add_suffix('1'),\n",
    "        on=\"variantid1\"\n",
    "    )\n",
    "    .merge(\n",
    "        test_etl\n",
    "        .add_suffix('2'),\n",
    "        on=\"variantid2\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:32:25.287137Z",
     "start_time": "2023-05-19T06:32:10.342926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features[[\"pic_dist_0_perc\", \"pic_dist_25_perc\", \"pic_dist_50_perc\"]] = (\n",
    "    test_features[[\"main_pic_embeddings_resnet_v11\", \"main_pic_embeddings_resnet_v12\"]].apply(\n",
    "        lambda x: pd.Series(get_pic_features_func(*x)), axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "test_features[[\"euclidean_name_bert_dist\", \"cosine_name_bert_dist\"]] = (\n",
    "    test_features[[\"name_bert_641\", \"name_bert_642\"]].apply(\n",
    "        lambda x: pd.Series(text_dense_distances(*x)), axis=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:32:34.415102Z",
     "start_time": "2023-05-19T06:32:34.354763Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features[\"cat3\"] = test_features[\"categories1\"].apply(lambda x: json.loads(x)[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:32:37.601571Z",
     "start_time": "2023-05-19T06:32:37.596721Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:32:40.679595Z",
     "start_time": "2023-05-19T06:32:40.668656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cat3_counts = test_features[\"cat3\"].value_counts().to_dict()\n",
    "\n",
    "cntr = 0\n",
    "for cat3 in test_cat3_counts:\n",
    "    if test_cat3_counts[cat3] < 50:\n",
    "        cntr += test_cat3_counts[cat3]\n",
    "        \n",
    "cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:34:07.170273Z",
     "start_time": "2023-05-19T06:34:07.157474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features[\"cat3_grouped\"] = test_features[\"cat3\"].apply(lambda x: x if test_cat3_counts[x] > 50 else \"rest\")\n",
    "# btw you can rename to `rest` the same categories that were assigned to this group in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:34:11.446732Z",
     "start_time": "2023-05-19T06:34:11.382287Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_example = test_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:34:14.146730Z",
     "start_time": "2023-05-19T06:34:14.086525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_example[\"scores\"] = model.predict_proba(test_features[feats])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:34:17.373009Z",
     "start_time": "2023-05-19T06:34:17.357230Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_example = submission_example[[\"variantid1\", \"variantid2\", \"scores\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:35:41.877989Z",
     "start_time": "2023-05-19T06:35:41.794727Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_example.drop_duplicates().merge(\n",
    "    test_features[[\"variantid1\", \"variantid2\"]].drop_duplicates([\"variantid1\", \"variantid2\"]),\n",
    "    on=[\"variantid1\", \"variantid2\"]\n",
    ").to_csv(\"submission_example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload your submission to leaderboard :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations \n",
    "\n",
    "- Work with names, in electronics names often contain a lot of useful information for matching.\n",
    "- Don't forget about attributes: working with it will allow your model to better distinguish matches from non-matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
