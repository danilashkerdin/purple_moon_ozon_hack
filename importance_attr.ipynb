{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b35d9-b07a-461e-8991-aa87306537a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be745827-a0c0-4cd9-bf7e-57d8a90d4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_pairs = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_pairs.parquet\")\n",
    "products_all = pd.read_parquet(\"hackathon_files_for_participants_ozon/train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddf8b2-026f-4989-ae85-12f8a679eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_all = products_all[\n",
    "    ['name', 'categories', 'characteristic_attributes_mapping', 'variantid']\n",
    "].dropna()\n",
    "products_all.categories = products_all.categories.apply(lambda x: json.loads(x)[\"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf04f2-7512-4d8d-86d7-67ef3e404725",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe4f2b-c406-4af0-8f89-f7934a492cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_pairs = var_pairs.merge(\n",
    "    products_all.add_suffix('1'),\n",
    "    on=\"variantid1\"\n",
    ").merge(\n",
    "    products_all.add_suffix('2'),\n",
    "    on=\"variantid2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3c92e-29f6-4463-bdcf-df12a4bb8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "identcial_pairs = product_pairs[product_pairs['target'] == 1]\n",
    "various_pairs = product_pairs[product_pairs['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cdc67-c050-43e3-b1bd-4def9a927f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "identcial_pairs.shape, various_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7145f7-6721-4725-8051-876efa410be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_freq = collections.Counter(\n",
    "    attr_name \n",
    "    for attributes1, attributes2 in zip(product_pairs.characteristic_attributes_mapping1,\n",
    "                                        product_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1.lower())) & set(json.loads(attributes2.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772f4f2-d2ce-43cb-af24-a9fc5005bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "   \n",
    "count_not_matched_attr_for_various_pairs = collections.Counter(\n",
    "    attr_name \n",
    "    if (\n",
    "        (\n",
    "            (' '.join(json.loads(attributes1.lower())[attr_name])) != (\n",
    "             ' '.join(json.loads(attributes2.lower())[attr_name]))\n",
    "        ) and (\n",
    "            cosine_similarity(\n",
    "                *vectorizer.fit_transform(\n",
    "                    [\n",
    "                        ' '.join(json.loads(attributes1.lower())[attr_name]) + 'tr',\n",
    "                        ' '.join(json.loads(attributes2.lower())[attr_name]) + 'tr'\n",
    "                    ]\n",
    "                )\n",
    "            ) < 0.5\n",
    "        )\n",
    "    ) else 'matched'\n",
    "    for (attributes1, attributes2) in zip(various_pairs.characteristic_attributes_mapping1, \n",
    "                                          various_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1.lower())) & set(json.loads(attributes2.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7948b-265a-4b5b-8007-21d05b28ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "del count_not_matched_attr_for_various_pairs['matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e866a-e0e7-4e91-b06b-d565c44035d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count_not_matched_attr_for_ident_pairs = collections.Counter(\n",
    "    attr_name \n",
    "      if (\n",
    "        (\n",
    "            (' '.join(json.loads(attributes1.lower())[attr_name])) != (\n",
    "             ' '.join(json.loads(attributes2.lower())[attr_name]))\n",
    "        ) and (\n",
    "            cosine_similarity(\n",
    "                *vectorizer.fit_transform(\n",
    "                    [\n",
    "                        ' '.join(json.loads(attributes1.lower())[attr_name]) + 'tr',\n",
    "                        ' '.join(json.loads(attributes2.lower())[attr_name]) + 'tr'\n",
    "                    ]\n",
    "                )\n",
    "            ) < 0.5\n",
    "        )\n",
    "    ) else 'matched'\n",
    "\n",
    "    for (attributes1, attributes2) in zip(identcial_pairs.characteristic_attributes_mapping1, \n",
    "                                          identcial_pairs.characteristic_attributes_mapping2)\n",
    "        for attr_name in set(json.loads(attributes1.lower())) & set(json.loads(attributes2.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd9a27-e207-421b-af18-b7dbae9ab556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del count_not_matched_attr_for_ident_pairs['matched']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71eb50-e579-4c9a-8d28-20b51d4d902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "count_not_matched_attr_for_ident_pairs_norm = {\n",
    "    attr: freq / attr_freq[attr]\n",
    "    for attr, freq in count_not_matched_attr_for_indent_pairs.items()\n",
    "}\n",
    "count_not_matched_attr_for_ident_pairs_norm = dict(\n",
    "    sorted(count_not_matched_attr_for_ident_pairs_norm.items(),\n",
    "           key=lambda x: x[1],\n",
    "           reverse=True)\n",
    ")\n",
    "count_not_matched_attr_for_various_pairs_norm = {\n",
    "    attr: freq / attr_freq[attr]\n",
    "    for attr, freq in count_not_matched_attr_for_various_pairs.items()\n",
    "}\n",
    "count_not_matched_attr_for_various_pairs_norm = dict(\n",
    "    sorted(count_not_matched_attr_for_various_pairs_norm.items(),\n",
    "           key=lambda x: x[1],\n",
    "           reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864d4ec-16e2-46a5-bf14-e39ca8ecdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(count_not_matched_attr_for_various_pairs_norm.keys())[:50]) & set(list(count_not_matched_attr_for_ident_pairs_norm.keys())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411dc7d-b933-4922-bbde-522e4f545347",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax.barh(\n",
    "    list(count_not_matched_attr_for_indent_pairs_norm.keys())[:50], \n",
    "    list(count_not_matched_attr_for_indent_pairs_norm.values())[:50]\n",
    ")\n",
    "fig.savefig('./count_not_matched_attr_for_indent_pairs_norm.png')\n",
    "\n",
    "fig2, (ax2) = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "ax2.barh(\n",
    "    list(count_not_matched_attr_for_various_pairs_norm.keys())[:50], \n",
    "    list(count_not_matched_attr_for_various_pairs_norm.values())[:50]\n",
    ")\n",
    "fig2.savefig('./count_not_matched_attr_for_various_pairs_norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813ef2-be9f-43e1-abaf-f25b109f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_not_matched_attr_for_various_pairs.json', 'w') as f:\n",
    "    json.dump(count_not_matched_attr_for_various_pairs, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eecf9-a070-4ecd-a4e8-4fa17ea51443",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('count_not_matched_attr_for_indent_pairs.json', 'w') as f:\n",
    "    json.dump(count_not_matched_attr_for_indent_pairs, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd188e-18ac-40b2-bad3-4fd073c5cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = (\n",
    "    ' '.join(json.loads(products_all.loc[i].characteristic_attributes_mapping.lower())[attr])\n",
    "    for i in products_all.index\n",
    "        for attr in json.loads(products_all.loc[i].characteristic_attributes_mapping.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810da79-951e-42d7-b715-e2ff5441b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnd = np.random.choice(various_pairs.index)\n",
    "\n",
    "attrs = set(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())) & set(\n",
    "    json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping2.lower())\n",
    ")\n",
    "\n",
    "if any(attrs):\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    a = {\n",
    "        attr:  cosine_similarity(\n",
    "            *vectorizer.fit_transform(\n",
    "                [\n",
    "                    ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]),\n",
    "                    ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr])\n",
    "                ]\n",
    "            )\n",
    "        ) for attr in attrs if len(\n",
    "            ' '.join(json.loads(product_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr])\n",
    "        ) > 3\n",
    "    }\n",
    "\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9304da34-e484-4414-83fd-0692ef101954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dbf8082-3301-4d2e-aa84-e13d74de82af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m cosine_similarity(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[43mTfidfVectorizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3 гб\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2 гб\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1401\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1401\u001b[0m X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:1253\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1251\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    *TfidfVectorizer(\n",
    "        stop_words=[],\n",
    "        min_df=0.1,\n",
    "        max_df=0.2\n",
    "    ).fit_transform(['3 гб', '2 гб'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a524500-afa5-4438-a5aa-cd39650338d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    vectorizer = TfidfVectorizer(min_df=0.6)\n",
    "\n",
    "    for attr in attrs:\n",
    "        print(' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]), '+++',\n",
    "              ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr]))\n",
    "        print(vectorizer.fit_transform(\n",
    "                [\n",
    "                    ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping1.lower())[attr]),\n",
    "                    ' '.join(json.loads(identcial_pairs.loc[rnd].characteristic_attributes_mapping2.lower())[attr])\n",
    "                ]\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70828d08-db1e-461a-8a2e-a743570c8f82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
